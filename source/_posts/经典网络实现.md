---
title: ç»å…¸ç½‘ç»œå®ç°
mathjax: true
date: 2021.2.20
author: dummerfu
authorLink: dummerfu.tk
categories: æŠ€æœ¯
comments: true
tags: æœºå™¨å­¦ä¹ 
photos: >-
  https://cdn.jsdelivr.net/gh/dummerchen/My_Image_Bed02@image_bed_001/img/20210317004422.jpg
abbrlink: 35344
description:
keywords:
---

## å‰è¨€

<div class="tip error">çœ‹ä¸æ‡‚åˆ«äººçš„ä»£ç ,è‡ªå·±å®ç°ä¸€éç»å…¸ç½‘ç»œ,ç†Ÿæ‚‰keras api</div>

 <div class="tip success"> æ°´ç¯‡åšå®¢ </div>

<div class="tip warning">æ²¡æœ‰è®­ç»ƒæµ‹è¯•è¿‡ç½‘ç»œçš„æ•ˆæœ,ç›´æ¥æ‹¿å»ç”¨å¯èƒ½ä¼šå‡ºé—®é¢˜!!!</div>

æœ¬æ„æ˜¯äº†è§£å¦‚ä½•è‡ªå·±æ„å»ºç½‘ç»œ,ä»¥é˜²æ—¥åçš„æ¨¡å‹è¿ç§»è¦å†å­¦ä¸€é. ~~ä¸è¦é—®ä¸ºä»€ä¹ˆæˆ‘çŸ¥é“è¦é‡å­¦~~

å¯èƒ½ç½‘ç»œä¼šæœ‰é”™è¯¯,ä½†æ˜¯æ— ä¼¤å¤§é›…,çŸ¥é“å¦‚ä½•æ„å»ºå°±è¡Œ ~~åæ­£ä»¥åç»å…¸ç½‘ç»œå¯ä»¥ç›´æ¥å¯¼å…¥~~

ä¸è¿‡åœ¨ä¹‹å‰è¦å…ˆäº†è§£ä¸€ä¸‹æ¨¡å‹ä¿å­˜ä¸åŒæ ¼å¼çš„åŒºåˆ«~~ä»¥é˜²æ¨¡å‹å®ç°äº†ä¸ä¼šä¿å­˜~~

## æ¨¡å‹ä¿å­˜

[TFå®˜ç½‘](https://tensorflow.google.cn/guide/keras/save_and_serialize?hl=zh-cn)

### Save_modelæ ¼å¼

è¿™ä¸ªæ˜¯æœ€ç®€å•ç²—æš´çš„æ¨¡å‹ä¿å­˜æ–¹æ³•äº†ã€‚

ä¿å­˜çš„æ¨¡å‹å°†åŒ…æ‹¬ï¼š

- æ¨¡å‹çš„æ¶æ„/é…ç½®
- æ¨¡å‹çš„æƒé‡å€¼ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ï¼‰
- æ¨¡å‹çš„ç¼–è¯‘ä¿¡æ¯ï¼ˆå¦‚æœè°ƒç”¨äº† `compile()`ï¼‰
- ä¼˜åŒ–å™¨åŠå…¶çŠ¶æ€ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œä½¿æ‚¨å¯ä»¥ä»ä¸Šæ¬¡ä¸­æ–­çš„ä½ç½®é‡æ–°å¼€å§‹è®­ç»ƒï¼‰





```python
# ä¿å­˜ä¸ºdirname_pathè·¯å¾„ä¸‹æ–‡ä»¶åä¸ºdirnameçš„æ–‡ä»¶å¤¹

model.save(dirname_path)
```



### H5æ ¼å¼



> Keras è¿˜æ”¯æŒä¿å­˜å•ä¸ª HDF5 æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æ¨¡å‹çš„æ¶æ„ã€æƒé‡å€¼å’Œ `compile()` ä¿¡æ¯ã€‚å®ƒæ˜¯ SavedModel çš„è½»é‡åŒ–æ›¿ä»£é€‰æ‹©ã€‚

ä½†æ˜¯åŒæ—¶å› ä¸ºåªæœ‰ä¸€ä¸ªh5æ–‡ä»¶ä¸ SavedModel æ ¼å¼ç›¸æ¯”ï¼ŒH5 æ–‡ä»¶ä¸åŒ…æ‹¬ä»¥ä¸‹ä¸¤æ–¹é¢å†…å®¹ï¼š

- é€šè¿‡ `model.add_loss()` å’Œ `model.add_metric()` æ·»åŠ çš„**å¤–éƒ¨æŸå¤±å’ŒæŒ‡æ ‡**ä¸ä¼šè¢«ä¿å­˜ï¼ˆè¿™ä¸ SavedModel ä¸åŒï¼‰ã€‚å¦‚æœæ‚¨çš„æ¨¡å‹æœ‰æ­¤ç±»æŸå¤±å’ŒæŒ‡æ ‡ä¸”æ‚¨æƒ³è¦æ¢å¤è®­ç»ƒï¼Œåˆ™æ‚¨éœ€è¦åœ¨åŠ è½½æ¨¡å‹åè‡ªè¡Œé‡æ–°æ·»åŠ è¿™äº›æŸå¤±ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸é€‚ç”¨äºé€šè¿‡ `self.add_loss()` å’Œ `self.add_metric()` åœ¨å±‚å†…åˆ›å»ºçš„æŸå¤±æŒ‡æ ‡ã€‚åªè¦è¯¥å±‚è¢«åŠ è½½ï¼Œè¿™äº›æŸå¤±å’ŒæŒ‡æ ‡å°±ä¼šè¢«ä¿ç•™ï¼Œå› ä¸ºå®ƒä»¬æ˜¯è¯¥å±‚ `call` æ–¹æ³•çš„ä¸€éƒ¨åˆ†ã€‚
- å·²ä¿å­˜çš„æ–‡ä»¶ä¸­**ä¸åŒ…å«è‡ªå®šä¹‰å¯¹è±¡ï¼ˆå¦‚è‡ªå®šä¹‰å±‚ï¼‰çš„è®¡ç®—å›¾**ã€‚

```python
# åªéœ€è¦åœ¨æ–‡ä»¶åååŠ .h5åç¼€å³å¯
model.save(name.h5)
```

###ã€€checkpoints



### ä¿å­˜æ—¶é™„å¸¦ç­¾å

```python
class Model(tf.keras.Model):

    @tf.function
    def call(self, x):
      ...

  m = Model()
  tf.saved_model.save(
      m, '/tmp/saved_model/',
      signatures=m.call.get_concrete_function(
          tf.TensorSpec(shape=[None, 3], dtype=tf.float32, name="inp")))

```



## ResNet

### æ‰‹å†™å®ç°

ä¸åŒçš„ResNetåªæœ‰ç»“æ„ä¸åŒ,unitæ˜¯ç›¸åŒçš„åªéœ€è¦æ”¹å˜layer_dimså°±å¯ä»¥å®ç°äº†

è¿™é‡Œä½¿ç”¨é‡å†™ç±»æ¥æ„å»ºç½‘ç»œ,è™½ç„¶è¦å†™å‰å‘ä¼ æ’­æ¯”è¾ƒéº»çƒ¦,ä½†æ˜¯è‡ªç”±åº¦æ›´é«˜

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
from  tensorflow import keras
import tensorboard
from tensorflow.keras import layers
import datetime
from matplotlib import pyplot as plt


BATCHSIZE=64

class BasicBlock(layers.Layer):
    def __init__(self,filter_num,stride=1):
        super(BasicBlock,self).__init__()
        self.conv1=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=stride,padding='same')
        self.bn1=layers.BatchNormalization()
        self.relu=layers.Activation('relu')

        self.conv2=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=1,padding='same')
        self.bn2 = layers.BatchNormalization()

        if(stride!=1):
            self.downsample = keras.Sequential()
            self.downsample.add(layers.Conv2D(filters=filter_num,kernel_size=(1,1),strides=stride))
        else:
            self.downsample=lambda x:x

    def call(self,inputs,training=None):

        out=self.conv1(inputs)
        out=self.bn1(out)
        out=self.relu(out)
        out=self.conv2(out)
        out=self.bn2(out)

        identity=self.downsample(inputs)
        output=layers.add([out,identity])
        output=self.relu(output)
        return output

class ResNet(keras.Model):
    def __init__(self,layer_dims,num_classes=100):
        super(ResNet, self).__init__()
        # self.flatten=layers.Flatten(input_shape=(32,32,3))
        self.stem=keras.Sequential([
            layers.Conv2D(64,(3,3),strides=(1,1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='same')
        ])
        self.layer1=self.build_resblock(filter_num=64,blocks=layer_dims[0])
        self.layer2=self.build_resblock(filter_num=128,blocks=layer_dims[1],stride=2)
        self.layer3=self.build_resblock(filter_num=256,blocks=layer_dims[2],stride=2)
        self.layer4=self.build_resblock(filter_num=512,blocks=layer_dims[3],stride=2)

        self.avgpool=layers.GlobalAveragePooling2D()
        self.fc=layers.Dense(num_classes)

    def call(self,inputs,training=None):
        # x = tf.reshape(inputs, [-1, 32 * 32*3])
        out=self.stem(inputs)
        out=self.layer1(out)
        out=self.layer2(out)
        out=self.layer3(out)
        out=self.layer4(out)
        out=self.avgpool(out)
        out=self.fc(out)

        return out
    def build_resblock(self,filter_num,blocks,stride=1):
        res_block=keras.Sequential()
        res_block.add(BasicBlock(filter_num,stride))

        for _ in range(1,blocks):
            res_block.add(BasicBlock(filter_num,stride=1))
        return  res_block

def resnet18():
    return ResNet(layer_dims=[2,2,2,2])

def preprocess(x,y):
    print('pre:', x.shape, y.shape)
    x=tf.cast(x,dtype=tf.float32)/255.0
    y=tf.cast(y,dtype=tf.int32)
    y = tf.squeeze(y)
    y=tf.one_hot(y,depth=100)

    print('after', x.shape, y.shape)

    return x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def train_model(train_db,val_db,is_train=False):
    model = resnet18()

    model.build(input_shape=(None, 32, 32, 3))
    model.summary()
    x=tf.random.normal([4,32,32,3])
    out=model(x)
    print(out.shape)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'],
    )


    path=os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard=keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db,validation_data=val_db,validation_freq=1,epochs=5,callbacks=[tensorboard])

        model.save_weights('./resnet18.h5')


def main():
    (x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()

    l=int(len(x)*0.8)
    train_db=data2tensor(x[:l],y[:l])
    val_db=data2tensor(x[l:],y[l:])
    test_db=data2tensor(x_test,y_test)

    # sample=next(iter(train_db))
    # print(sample[0].shape,sample[1].shape)
    # plt.imshow(sample[0])
    # plt.show()
    train_model(train_db,val_db,is_train=False)

main()
```

### è‡ªå¸¦apiå®ç°

å› ä¸ºapplicationé‡Œé¢éƒ½æœ‰ï¼ŒåŠŸèƒ½éƒ½ç±»ä¼¼æ•…åé¢ä¸å†èµ˜è¿°

<div class="tips warning"> tf.kears.application.resnet50.Resnet50ä¸tf.kears.application.Resnet50çš„åŠŸèƒ½éƒ½ä¸€æ ·</div>



```python
from tensorflow.keras.applications import *

model=ResNet50(weights='./resnet50_weights_tf_dim_ordering_tf_kernels.h5')
path='./cat.jpg'
# è¯»å…¥å›¾ç‰‡
image=image_preprocess.img_decoder(path)

pre1=model.predict(image)
# è¿™ä¸ªèƒ½ä½¿æ ‡ç­¾å¯¹åº”èµ·æ¥
pre=resnet50.decode_predictions(pre1)
print(pre)
```



## VGG

â€‹	è¿™é‡Œä½¿ç”¨kerasçš„é«˜çº§apiæ¥æ„å»ºç½‘ç»œ,å½“ç„¶ä½¿ç”¨Sequentialä¹Ÿå¯ä»¥å®ç°åŒæ ·çš„æ•ˆæœ.

```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/19 20:48

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import datetime
import  tensorflow as tf
from  tensorflow import  keras


BATCHSIZE=32


def preprocess(x,y):
    print('pre',x.shape,y.shape)

    x=2*tf.cast(x,dtype=tf.float32)/255.0 -1
    y = tf.squeeze(y)
    y=tf.cast(y,dtype=tf.int32)
    y=tf.one_hot(y,depth=100)
    print('after:',x.shape,y.shape)
    return  x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def VGG(image_shape,n_class):

    print(image_shape[0],image_shape[1],image_shape[2])
    inputs = keras.Input(shape=[image_shape[0],image_shape[1],image_shape[2]])

    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(inputs)
    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x=keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Flatten()(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)

    outputs= keras.layers.Dense(n_class, activation=keras.activations.softmax, use_bias=True)(x)
    # åŸºäºModelæ–¹æ³•æ„å»ºæ¨¡å‹
    model = keras.Model(inputs=inputs, outputs=outputs)

    return model

def train(train_db,val_db,is_train=False):
    model = VGG([32,32,3],n_class=100)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(),
        metrics=['accuracy'],
    )

    path = os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db, validation_data=val_db, validation_freq=1, epochs=5, callbacks=[tensorboard])

        model.save_weights('./vgg16.h5')


(x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()
print('pre',x.shape,y.shape)

train_db=data2tensor(x,y)
test_db=data2tensor(x_test,y_test)

train(train_db=train_db,val_db=test_db,is_train=False)

```



## LSTM



<div class="tip warning">layers.lstmcellå’Œlayers.lstmä¼ å‚æ˜¯ä¸ä¸€æ ·çš„</div>

å‰è€…éœ€è¦æ‰‹åŠ¨æ›´æ–°stateå‚æ•°($h_{t-1}$,$c_{t-1}$)ä½†æ˜¯åè€…è‡ªåŠ¨æ›´æ–°ï¼Œå¦‚æœéœ€è¦å¤šå±‚å åŠ åˆ™éœ€è¦è®¾ç½®return_sequence=True , unroll=True



```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/21 17:46

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt

mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

import tensorflow as tf
from tensorflow import keras

# æœ€å¸¸è§çš„å‰20000ä¸ªå•è¯
max_features=20000

# ä¸€å¥è¯çš„æœ€å¤§é•¿åº¦
max_len=100
batchsize=64

class Mylstm(keras.Model):
    def __init__(self,units):
        super(Mylstm,self).__init__()

        # [b,100] => [b,100,100]

        self.embeding=keras.layers.Embedding(input_dim=max_features,input_length=max_len,output_dim=100)
        self.rnn=keras.Sequential([
            keras.layers.LSTM(units=units,dropout=0.5,return_sequences=True,unroll=True),
            keras.layers.LSTM(units=units,dropout=0.5,unroll=True)
        ])

        self.fc=keras.layers.Dense(1,activation=keras.activations.sigmoid)

    def call(self,inputs,training=None):

        # [b,100] => [b,100,100]
        x=self.embeding(inputs)
        print(x.shape)
        # [b,100,100] => [b,64]
        x=self.rnn(x)
        x=self.fc(x)

        return x


def data2tensor(x,y):
    x=keras.preprocessing.sequence.pad_sequences(sequences=x,maxlen=max_len)
    x=tf.cast(x,dtype=tf.int32)
    y=tf.cast(y,dtype=tf.int32)

    print(x.shape,y.shape)

    db=tf.data.Dataset.from_tensor_slices((x,y)).shuffle(10000).batch(batchsize,drop_remainder=True)
    return db

def train(db_train,db_val,db_test):

    model=Mylstm(64)
    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.BinaryCrossentropy(),
        metrics=['accuracy'],
    )
    model.fit(db_train,epochs=5,validation_data=db_val,validation_freq=1)

    model.evaluate(db_test)
    return

def main():
    (x,y),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=max_features)

    l=int(len(x)*0.8)
    db_train=data2tensor(x[:l],y[:l])
    db_val=data2tensor(x[l:],y[l:])
    db_test=data2tensor(x_test,y_test)


    train(db_train,db_val,db_test)


if __name__ == "__main__":
    main()

```

## AutoEncoder|VAE

*è¿™é‡Œæ˜¯è‡ªå®šä¹‰è®­ç»ƒï¼Œå½“ç„¶ç›¸æ¯”ä¹‹ä¸‹æ›´å¤æ‚ä½†æ˜¯è‡ªç”±åº¦ä¹Ÿæ›´é«˜ã€‚*

autoencoderå°±æ˜¯ä¸¤ä¸ªè‡ªå®šä¹‰ç½‘ç»œï¼Œå…ˆé™ç»´å¾—åˆ°ç‰¹å¾å‘é‡hï¼Œå†å‡åˆ°åŸæœ¬ç»´åº¦å°±è¡Œäº†~~æ²¡ä»€ä¹ˆæŠ€æœ¯å«é‡ï¼Œå°±ä¸å†™äº†~~ï¼Œå…³é”®æ˜¯å®ƒçš„æ€è·¯éå¸¸å…·æœ‰å¯å‘æ€§ã€‚



è¿™é‡Œè¦æ³¨æ„çš„æ˜¯mean,var Denseæ˜¯ä¸¤ä¸ªDenseï¼Œå³ä½¿è®¡ç®—æ–¹å¼ä¸€æ ·ä½†æ˜¯è¦ç”¨ä¸¤Dense,å¦‚æœä¸€ä¸ªDenseç®—ä¸¤æ¬¡å› ä¸ºæƒé‡çš„åŸå› ç»“æœæ˜¯ç›¸åŒçš„ï¼Œç›´æ¥ä¼šå¯¼è‡´å›¾ç‰‡è¶Šæ¥è¶Šæš—ã€‚

å…ˆé™„ä¸Š[tf.nnçš„å‡ ç§æŸå¤±å‡½æ•°åŒºåˆ«](https://www.cnblogs.com/henuliulei/p/13742376.html)å†é™„ä»£ç 

```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/22 21:55

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import matplotlib as mpl
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from PIL import Image
import numpy as np
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

tf.random.set_seed(2345)
# autoencoder è®¡ç®—é‡å¾ˆå°batchå¯ä»¥å¤§ä¸€ç‚¹
batch_size=512
# ç‰¹å¾ç»´æ•°
z_dims=20


class VAE(keras.Model):
    def __init__(self):
        super(VAE,self).__init__()

        #encoder
        self.encoder=keras.Sequential([
            keras.layers.InputLayer(input_shape=(28*28)),
            keras.layers.Dense(128),
        ])

        self.meanfc=keras.layers.Dense(z_dims)
        self.varfc=keras.layers.Dense(z_dims)
        #decoder
        self.decoder=keras.Sequential([
            keras.layers.Dense(128, activation=tf.nn.relu),
            keras.layers.Dense(784),
        ])

    def reparamize(self,mean,log_var):

        eps=tf.random.normal(log_var.shape)
        z=mean+eps*tf.exp(log_var*0.5)
        return z

    def call(self,inputs,training=None):
        h=self.encoder(inputs)
        
        mean=self.meanfc(h)
        log_var=self.varfc(h)
        
        z=self.reparamize(mean,log_var)
        
        outputs=self.decoder(z)

        return outputs,mean,log_var

def data2tensor(x,y):

    x=tf.cast(x,dtype=tf.float32)/255.0
    db=tf.data.Dataset.from_tensor_slices(x)
    db=db.shuffle(batch_size*5).batch(batch_size)
    return db

def save_images(imgs,name):
    new_im = Image.new('L', (280, 280))

    index = 0
    for i in range(0, 280, 28):
        for j in range(0, 280, 28):
            im = imgs[index]
            im = Image.fromarray(im, mode='L')
            new_im.paste(im, (i, j))
            index += 1
    new_im.save(name)

def train_and_test(db_train,db_test):

    model=VAE()
    # model.build(input_shape=(4,784))
    optimizer=tf.optimizers.Adam()
    for epoch in range(100):
        for step,x in enumerate(db_train):
            # print(x.shape)
            x=tf.reshape(x,[-1,784])

            with tf.GradientTape() as tape:
                x_hat,mean,log_var=model(x)
                # è¿™é‡Œä½¿ç”¨çš„è¿™ä¸ªlossæ˜¯ä¸ºäº†æ›´å¥½çš„æ”¶æ•›ï¼Œä½¿ç”¨å…¶ä»–çš„ä¹Ÿè¡Œï¼Œä½†æ˜¯è¦å¤šè®­ç»ƒ
                redu_loss=tf.nn.sigmoid_cross_entropy_with_logits(x,x_hat)

                # è¿™é‡Œå…¶å®éšä¾¿ï¼Œreduce_mean(),reduce_sum()åº”è¯¥éƒ½è¡Œåæ­£éƒ½æ˜¯minimize loss
                # reduce_mean()å’Œreduce_sum()|reduce_sum/x.shape[0]è®­ç»ƒç»“æœå®Œå…¨ä¸åŒ..
                # ä½†æ˜¯åä¸¤è€…ç›¸ä¼¼
                redu_loss=tf.reduce_sum(redu_loss)/x.shape[0]

                kl=-0.5*(log_var+1-mean**2-tf.exp(log_var))
                # prekl=tf.reduce_mean(kl)
                kl=tf.reduce_sum(kl)/x.shape[0]
     
                loss=redu_loss+kl*1.0
            grads=tape.gradient(loss,model.trainable_variables)

            optimizer.apply_gradients(zip(grads,model.trainable_variables))
            if step%50==0:

                print(epoch,step,"kl_loss:",kl,'loss:',loss,'x_shape0',x.shape[0])
        # evaluation
        z=tf.random.normal((batch_size,z_dims))
        sample_x=model.decoder(z)
        sample_x=tf.nn.sigmoid(sample_x)
        sample_x = tf.reshape(sample_x, [-1, 28, 28]).numpy() * 255.
        sample_x= sample_x.astype(np.uint8)

        save_images(sample_x, 'vae_images/sample_epoch_%d.png' % epoch)


        test_x = next(iter(db_test))
        test_x,_,_= model(tf.reshape(test_x, [-1, 784]))
        # [b, 784] => [b, 28, 28]
        test_x=tf.nn.sigmoid(test_x)
        test_x = tf.reshape(test_x, [-1, 28, 28])

        # [b, 28, 28] => [2b, 28, 28]
        test_x= test_x.numpy() * 255.
        test_x = test_x.astype(np.uint8)
        save_images(test_x, 'vae_images/test_epoch_%d.png' % epoch)

    model.save_weights('./vae.h5')

if __name__ == "__main__":
    (x,y),(x_test,y_test)=keras.datasets.mnist.load_data()

    l=int(len(x)*0.8)
    print(x.shape, y.shape,l,28*28)
    db_train=data2tensor(x[:l],y[:l])
    db_val=data2tensor(x[l:],y[l:])
    db_test=data2tensor(x_test,y_test)

    train_and_test(db_train,db_val)

```

## Gan

### WGANåŸç†

GANä¸€ç›´é¢ä¸´ç€G,Dè®­ç»ƒå›°éš¾ã€G,Dçš„æŸå¤±å‡½æ•°ä¸è®­ç»ƒå¥½åæ— å…³(ç”±äºjsæ•£åº¦ï¼Œloss å¸¸å¸¸æ˜¯log2)ç­‰é—®é¢˜ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šä¾¿æå‡ºäº†WGANï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„GAN,WGANåªåšäº†å‡ ç‚¹æ”¹åŠ¨ç¡®æœ‰å¾ˆå¥½çš„æ•ˆæœ

* Dçš„æœ€åä¸€å±‚å»æ‰sigmod
* G,Dlossä¸å–log
* æ¯æ¬¡æ›´æ–°Dçš„å‚æ•°ååšä¸€ä¸ªæ¢¯åº¦æƒ©ç½šï¼ˆgradient penaltyï¼‰

GANçš„åŸæœ¬æŸå¤±å‡½æ•°ä¸º
$$
E_{z \in p_z(z)}[log(1-D(G(z)))]
$$
ä½†æ˜¯è¿™æ ·å¯¼è‡´äº†å¦‚æœDå¤ªå¥½äº†Gåˆ™è®­ç»ƒä¸åˆ°æœ‰æ•ˆçš„æ¢¯åº¦ï¼ŒGå¤ªå¥½äº†Dåˆè®­ç»ƒä¸åˆ°æœ‰æ•ˆçš„æ¢¯åº¦

æ‰€ä»¥WGANçš„æŸå¤±å‡½æ•°æ”¹ä¸ºäº†ï¼Œåœ¨improve WGANä¸­è¿˜åŠ å…¥äº†gradient penalty
$$
E_{z\in p_z(z)}[-logD(G(z))] = KL(P_g||P_{data})-2JS(P_{data}||P_g)+\lambda gp
$$
WGANç†è®ºä¸Šç»™å‡ºäº†GANè®­ç»ƒä¸ç¨³å®šçš„åŸå› ï¼Œå³äº¤å‰ç†µï¼ˆJSæ•£åº¦ï¼‰ä¸é€‚åˆè¡¡é‡å…·æœ‰ä¸ç›¸äº¤éƒ¨åˆ†çš„åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œè½¬è€Œä½¿ç”¨wasserteinè·ç¦»å»è¡¡é‡ç”Ÿæˆæ•°æ®åˆ†å¸ƒå’ŒçœŸå®æ•°æ®åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œç†è®ºä¸Šè§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚

â€‹	WGANç›¸å¯¹äºDCGANï¼ŒWGANè™½ç„¶æ”¶æ•›æ—¶é—´æ›´é•¿ä½†æ˜¯æ›´ç¨³å®šï¼Œæ‰€ä»¥å¯¹äºæ›´å¤æ‚ç½‘ç»œæ¥è¯´æ›´å€¾å‘äºWGANï¼Œæ¯”å¦‚ä½¿ç”¨resnetå¯ä»¥è¾¾åˆ°æ›´å¥½çš„ç»“æœã€‚

### code

*generatoré‡Œçš„åå·ç§¯å‚æ•°å¿…é¡»æœ€åè¦è®¡ç®—ç»“æœèƒ½å»åˆDiscriminatorçš„input_shape*

å› ä¸ºè¦åœ¨batchnormåé¢åšæ¿€æ´»æ‰€ä»¥ä¸èƒ½åƒä¹‹å‰ä¸€æ ·åœ¨å·ç§¯å±‚é‡Œé¢æ¿€æ´»ã€‚

è¿™æ˜¯æˆ‘è‡ªå·±çš„wganè·‘3000ä¸ªepochåçš„ç»“æœï¼Œå¯ä»¥æ˜æ˜¾çœ‹å‡ºå­¦ä¹ åˆ°äº†å¤´å‘å’Œçœ¼ç›(ç›¸æ¯”ä¹‹ä¸‹åˆ«äººè°ƒçš„å‚å¤ªç‰›äº†)

![wg_img_3400](https://cdn.jsdelivr.net/gh/dummerchen/My_Image_Bed01@master/img/20210227002207.jpg)



æ•°æ®é›†kaggleä¸Šéšä¾¿æ‰¾~~å•Šï¼ŒkaggleçœŸé¦™ï¼Œå„æ–¹æ„ä¹‰ä¸Š~~ï¼Œ

é¾™ä¹¦é‡Œé¢çš„æåˆ°çš„æ•°æ®é›†åœ¨è¿™é‡Œhttps://pan.baidu.com/s/1Yn53uxFLCbja13_6Ay44MA 

æ•°æ®é›†æ¥æºåœ¨[è¿™é‡Œ](https://zhuanlan.zhihu.com/p/24767059),æˆ‘å¼€å§‹æ²¡çœ‹issueæ²¡æ‰¾åˆ°è¿™ä¸ªæ•°æ®é›†ï¼Œçˆ¬åˆ°ä¸€åŠæ‰çœ‹åˆ°ğŸ˜“ï¼Œ

æˆ‘çˆ¬çš„æ•°æ®åœ¨https://pan.baidu.com/s/1JsUHx_1blY6pGx0DQfE0nQ æå–ç ï¼š3621 ï¼ˆåªæœ‰ä¸‰ä¸‡å¼ å›¾ç‰‡â€¦

æˆ‘å†™çš„ganå‚æ•°å¤ªå·®äº†ï¼Œçœ‹é¾™ä¹¦è¯´è·‘3ä¸‡æ¬¡ä¼¼ä¹èƒ½å¾—åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Ÿç®—äº†ç›´æ¥ä¸Šåˆ«äººå·²ç»è°ƒå¥½å‚çš„WGANä»£ç å§ï¼ŒçŸ¥ä¹é‚£ä¸ªè°ƒå¥½å‚çš„DCGANå¤ªçŒ›äº†ï¼Œ300epochå±…ç„¶å°±æˆå‹äº†orzï¼ˆè™½ç„¶æˆ‘æ²¡è·‘

### trainå‡½æ•°

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import glob
import numpy as np
import matplotlib as mpl
from PIL import Image
import tensorflow as tf
from tensorflow import keras
import dataset
import  tensorflow as tf
from    tensorflow import keras
from    tensorflow.keras import layers

class Generator(keras.Model):

    def __init__(self):
        super(Generator, self).__init__()

        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]
        self.fc = layers.Dense(3*3*512)

        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')
        self.bn1 = layers.BatchNormalization()

        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')
        self.bn2 = layers.BatchNormalization()

        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')

    def call(self, inputs, training=None):
        # [z, 100] => [z, 3*3*512]
        x = self.fc(inputs)
        x = tf.reshape(x, [-1, 3, 3, 512])
        x = tf.nn.leaky_relu(x)

        #
        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))
        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))
        x = self.conv3(x)
        x = tf.tanh(x)

        return x


class Discriminator(keras.Model):

    def __init__(self):
        super(Discriminator, self).__init__()

        # [b, 64, 64, 3] => [b, 1]
        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')

        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')
        self.bn2 = layers.BatchNormalization()

        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')
        self.bn3 = layers.BatchNormalization()

        # [b, h, w ,c] => [b, -1]
        self.flatten = layers.Flatten()
        self.fc = layers.Dense(1)


    def call(self, inputs, training=None):

        x = tf.nn.leaky_relu(self.conv1(inputs))
        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))
        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))

        # [b, h, w, c] => [b, -1]
        x = self.flatten(x)
        # [b, -1] => [b, 1]
        logits = self.fc(x)

        return logits
def save_result(val_out, val_block_size, image_path, color_mode):
    def preprocess(img):
        img = ((img + 1.0) * 127.5).astype(np.uint8)
        # img = img.astype(np.uint8)
        return img

    preprocesed = preprocess(val_out)
    final_image = np.array([])
    single_row = np.array([])
    for b in range(val_out.shape[0]):
        # concat image into a row
        if single_row.size == 0:
            single_row = preprocesed[b, :, :, :]
        else:
            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)

        # concat image row to final_image
        if (b+1) % val_block_size == 0:
            if final_image.size == 0:
                final_image = single_row
            else:
                final_image = np.concatenate((final_image, single_row), axis=0)

            # reset single row
            single_row = np.array([])

    if final_image.shape[2] == 1:
        final_image = np.squeeze(final_image, axis=2) 
    Image.fromarray(final_image).save(image_path)


def celoss_ones(logits):
   	return - tf.reduce_mean(logits)


def celoss_zeros(logits):
    return tf.reduce_mean(logits)


def gradient_penalty(discriminator, batch_x, fake_image):

    batchsz = batch_x.shape[0]

    # [b, h, w, c]
    t = tf.random.uniform([batchsz, 1, 1, 1])
    # [b, 1, 1, 1] => [b, h, w, c]
    t = tf.broadcast_to(t, batch_x.shape)

    interplate = t * batch_x + (1 - t) * fake_image

    with tf.GradientTape() as tape:
        tape.watch([interplate])
        d_interplote_logits = discriminator(interplate, training=True)
    grads = tape.gradient(d_interplote_logits, interplate)

    # grads:[b, h, w, c] => [b, -1]
    grads = tf.reshape(grads, [grads.shape[0], -1])
    gp = tf.norm(grads, axis=1) #[b]
    gp = tf.reduce_mean( (gp-1)**2 )

    return gp



def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):
    # 1. treat real image as real
    # 2. treat generated image as fake
    fake_image = generator(batch_z, is_training)
    d_fake_logits = discriminator(fake_image, is_training)
    d_real_logits = discriminator(batch_x, is_training)

    d_loss_real = celoss_ones(d_real_logits)
    d_loss_fake = celoss_zeros(d_fake_logits)
    gp = gradient_penalty(discriminator, batch_x, fake_image)

    loss = d_loss_real + d_loss_fake + 10. * gp

    return loss, gp


def g_loss_fn(generator, discriminator, batch_z, is_training):

    fake_image = generator(batch_z, is_training)
    d_fake_logits = discriminator(fake_image, is_training)
    loss = celoss_ones(d_fake_logits)

    return loss


def main():

    tf.random.set_seed(233)
    np.random.seed(233)

    # hyper parameters
    z_dim = 100
    epochs = 3000000
    batch_size = 512
    learning_rate = 0.0005
    is_training = True


    img_path = glob.glob('.\animefacedataset\images\*.jpg')
    assert len(img_path) > 0
    

    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)
    print(dataset, img_shape)
    sample = next(iter(dataset))
    print(sample.shape, tf.reduce_max(sample).numpy(),
          tf.reduce_min(sample).numpy())
    dataset = dataset.repeat()
    db_iter = iter(dataset)


    generator = Generator() 
    generator.build(input_shape = (None, z_dim))
    discriminator = Discriminator()
    discriminator.build(input_shape=(None, 64, 64, 3))
    z_sample = tf.random.normal([100, z_dim])


    g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)
    d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)


    for epoch in range(epochs):
		# è®­ç»ƒ5æ¬¡discriminator å†è®­ç»ƒä¸€æ¬¡generatorï¼ï¼ï¼ä¸ç„¶å°±ä¼šå‡ºç°åƒæˆ‘ä¸€æ ·çš„å›¾
        for _ in range(5):
            batch_z = tf.random.normal([batch_size, z_dim])
            batch_x = next(db_iter)

            # train D
            with tf.GradientTape() as tape:
                d_loss, gp = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)
            grads = tape.gradient(d_loss, discriminator.trainable_variables)
            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))
        
        batch_z = tf.random.normal([batch_size, z_dim])

        with tf.GradientTape() as tape:
            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)
        grads = tape.gradient(g_loss, generator.trainable_variables)
        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))

        if epoch % 100 == 0:
            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss),
                  'gp:', float(gp))

            z = tf.random.normal([100, z_dim])
            fake_image = generator(z, training=False)
            img_path = os.path.join('images', 'wgan-%d.png'%epoch)
            save_result(fake_image.numpy(), 10, img_path, color_mode='P')

if __name__ == '__main__':
    main()
```

### datasetload å‡½æ•°

```python
import multiprocessing

import tensorflow as tf


def make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):

    # @tf.function
    def _map_fn(img):
        img = tf.image.resize(img, [resize, resize])
        # img = tf.image.random_crop(img,[resize, resize])
        # img = tf.image.random_flip_left_right(img)
        # img = tf.image.random_flip_up_down(img)
        img = tf.clip_by_value(img, 0, 255)
        img = img / 127.5 - 1 #-1~1
        return img

    dataset = disk_image_batch_dataset(img_paths,
                                          batch_size,
                                          drop_remainder=drop_remainder,
                                          map_fn=_map_fn,
                                          shuffle=shuffle,
                                          repeat=repeat)
    img_shape = (resize, resize, 3)
    len_dataset = len(img_paths) // batch_size

    return dataset, img_shape, len_dataset


def batch_dataset(dataset,
                  batch_size,
                  drop_remainder=True,
                  n_prefetch_batch=1,
                  filter_fn=None,
                  map_fn=None,
                  n_map_threads=None,
                  filter_after_map=False,
                  shuffle=True,
                  shuffle_buffer_size=None,
                  repeat=None):
    # set defaults
    if n_map_threads is None:
        n_map_threads = multiprocessing.cpu_count()
    if shuffle and shuffle_buffer_size is None:
        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048

    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly
    if shuffle:
        dataset = dataset.shuffle(shuffle_buffer_size)

    if not filter_after_map:
        if filter_fn:
            dataset = dataset.filter(filter_fn)

        if map_fn:
            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)

    else:  # [*] this is slower
        if map_fn:
            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)

        if filter_fn:
            dataset = dataset.filter(filter_fn)

    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)

    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)

    return dataset


def memory_data_batch_dataset(memory_data,
                              batch_size,
                              drop_remainder=True,
                              n_prefetch_batch=1,
                              filter_fn=None,
                              map_fn=None,
                              n_map_threads=None,
                              filter_after_map=False,
                              shuffle=True,
                              shuffle_buffer_size=None,
                              repeat=None):
    """Batch dataset of memory data.
    Parameters
    ----------
    memory_data : nested structure of tensors/ndarrays/lists
    """
    dataset = tf.data.Dataset.from_tensor_slices(memory_data)
    dataset = batch_dataset(dataset,
                            batch_size,
                            drop_remainder=drop_remainder,
                            n_prefetch_batch=n_prefetch_batch,
                            filter_fn=filter_fn,
                            map_fn=map_fn,
                            n_map_threads=n_map_threads,
                            filter_after_map=filter_after_map,
                            shuffle=shuffle,
                            shuffle_buffer_size=shuffle_buffer_size,
                            repeat=repeat)
    return dataset


def disk_image_batch_dataset(img_paths,
                             batch_size,
                             labels=None,
                             drop_remainder=True,
                             n_prefetch_batch=1,
                             filter_fn=None,
                             map_fn=None,
                             n_map_threads=None,
                             filter_after_map=False,
                             shuffle=True,
                             shuffle_buffer_size=None,
                             repeat=None):
    """Batch dataset of disk image for PNG and JPEG.
    Parameters
    ----------
        img_paths : 1d-tensor/ndarray/list of str
        labels : nested structure of tensors/ndarrays/lists
    """
    if labels is None:
        memory_data = img_paths
    else:
        memory_data = (img_paths, labels)

    def parse_fn(path, *label):
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3
        return (img,) + label

    if map_fn:  # fuse `map_fn` and `parse_fn`
        def map_fn_(*args):
            return map_fn(*parse_fn(*args))
    else:
        map_fn_ = parse_fn

    dataset = memory_data_batch_dataset(memory_data,
                                        batch_size,
                                        drop_remainder=drop_remainder,
                                        n_prefetch_batch=n_prefetch_batch,
                                        filter_fn=filter_fn,
                                        map_fn=map_fn_,
                                        n_map_threads=n_map_threads,
                                        filter_after_map=filter_after_map,
                                        shuffle=shuffle,
                                        shuffle_buffer_size=shuffle_buffer_size,
                                        repeat=repeat)

    return dataset
```

1000epochä¹‹åæ˜¯è¿™æ ·

![wgansample-1000](https://cdn.jsdelivr.net/gh/dummerchen/My_Image_Bed01@master/img/20210227110619.png)

## åè®°



â€‹	æœ¬æ¥æ˜¯æƒ³æ¯ä¸€ä¸ªç»å…¸ç½‘ç»œéƒ½è¯¦ç»†å†™çš„ï¼Œä½†æ˜¯æ„Ÿè§‰è¿™æ ·ä¼šå¯¼è‡´å¤ªä¸“ä¸šå…¨æ˜¯å…¬å¼ä¹Ÿä¸ä¼šæœ‰äººå»ä»”ç»†çœ‹ ~~å…¶å®æ˜¯æˆ‘ä¸ä¼šã€‚~~ç»“æœå˜æˆäº†ç°åœ¨è¿™ç§ç±»ä¼¼æ¿å­çš„ä¸œè¥¿ã€‚~~æ°´åšå®¢æ‰æ˜¯åŸåŠ¨åŠ›~~

â€‹	ç»ˆäºä½“ä¼šåˆ°ç”µè„‘çš„è‹¦äº†ï¼Œcpuå ç”¨ç‡*99%* è¿˜è¦å¼€å¤šçº¿ç¨‹åŒæ—¶çˆ¬å›¾ç‰‡â€¦ï¼ˆè™½ç„¶ç°åœ¨å­—éƒ½æ˜¾ç¤ºä¸å‡ºæ¥äº†

è¿™é‡Œå°±éšä¾¿æ€»ç»“ä¸€ä¸‹å­¦ä¹ çš„ç»éªŒï¼š

### ä»£ç æ–¹é¢

* keras.build(inputs_shape)ï¼šè¿™é‡Œæœ€å¥½æ˜¯ä½¿ç”¨tupleå½¢å¼è¡¨ç¤ºä¸ç„¶ä¼šæŠ¥å¥‡æ€ªçš„é”™ï¼Œtensorflowå’Œpytorchä¸åŒè¿™æ–¹é¢æ›´åŠ ä¸¥æ ¼ã€‚
* tf.lossesï¼š è¿™ä¸ªæ¨¡å—é‡Œçš„å‡½æ•°å¤§å°å†™ä¸åŒåŠŸèƒ½ä¹Ÿæ˜¯ä¸åŒçš„ï¼Œ~~å…·ä½“å¯ä»¥çœ‹å®˜ç½‘~~ï¼Œå¦‚æœç”¨complieå»ºè®®ç”¨å¤§å°çš„å‡½æ•°ï¼Œè‡ªå®šä¹‰ç®—lossä½¿ç”¨å°å†™çš„å‡½æ•°
* sigmodå’Œsoftmaxï¼š å½“'åˆ†ç±»'äº‹ç‰©ä¸å®Œå…¨ç›¸äº’ç‹¬ç«‹å¯ä»¥ä½¿ç”¨sigmodå¦åˆ™softmaxï¼Œsoftmaxä¸€å®šè¦onehot 
* model.saveï¼šè¿™ä¸ªå› ä¸ºä¿å­˜äº†ç½‘ç»œç»“æ„åªèƒ½ç”¨åœ¨çº¯è‡ªå®šä¹‰ç½‘ç»œé‡Œï¼Œç»§æ‰¿ç±»æ˜¯ä¸è¡Œçš„ã€‚
* layers.BatchNormalization:ï¼šè¿™ä¸ªå‡½æ•°æœ‰ä¸€ä¸ªtrainableå‚æ•°,train=True|Noneï¼Œtest=False|0,å…·ä½“å¯ä»¥çœ‹æºç è¯´æ˜ï¼Œä½†æ˜¯åƒä¸‡è¦è®¾ç½®æ­£ç¡®[åŸå› å¯å‚è€ƒè¿™é‡Œ](http://www.cainiaoxueyuan.com/suanfa/11644.html)
* layers.Flattenä¸Denseï¼šflattenåªæ˜¯å•çº¯çš„reshapeç»´åº¦æ˜¯å›ºå®šçš„ï¼ŒDenseè¿˜ä½œäº†ä¸€æ¬¡å…¨è¿æ¥



### ç½‘ç»œç­‰æ–¹é¢

â€‹	å¯ä»¥ä»ä»£ç ä¸­çœ‹å‡ºç°æœ‰çš„å‡ ç§ç½‘ç»œæ„å»ºæ ¼å¼ã€‚å½“åˆæˆ‘ä¹Ÿçº ç»“äº†è®¸å¤šï¼Œæœ€åè¿˜æ˜¯å‡†å¤‡ä½¿ç”¨ganç½‘ç»œçš„æ ¼å¼ï¼Œæ¯•ç«Ÿæ¡†æ¶å¥½ç”¨æ˜¯å¥½ç”¨ï¼Œä½†è¿™æ˜¯ç‰ºç‰²â€˜è‡ªç”±â€™æ¢æ¥çš„ï¼Œå¯¹åæœŸè‡ªä¸»æ„å»ºç½‘ç»œå¯èƒ½ä¼šèµ·åˆ°åæ•ˆæœã€‚~~æ¯ä¸ªäººå–œå¥½ä¸åŒï¼Œä¹Ÿä¸ç”¨å¤ªå‚è€ƒæˆ‘çš„å»ºè®®ã€‚~~

â€‹	kæŠ˜éªŒè¯ç­‰trickæ˜¯è§†é¢‘é‡Œæ²¡æœ‰è®²çš„ï¼ˆè§†é¢‘å‚è€ƒä¸‹é¢çš„å­¦ä¹ èµ„æºï¼‰ï¼Œå¯ä»¥è‡ªå·±å»çœ‹çœ‹ç›¸å…³trickã€‚

### å­¦ä¹ èµ„æº

~~æˆ‘æ‰ä¸æ˜¯çœ‹åˆ°Ganå¯ä»¥éšæœºç”Ÿæˆè€å©†æ‰æƒ³å­¦Gançš„~~

æ—¥æœˆå…‰åçš„ã€Štensorflowå…¥é—¨å­¦ä¹ ä¸å®æˆ˜çš„ã€‹èµ„æºå¼„ä¸åˆ°ï¼Œå¯æƒœäº†å…è´¹è¯¾ç¨‹è®²çš„ç¡®å®å¥½å°±æ˜¯å¤ªè´µäº†ã€‚

å°±è·Ÿç€[é¾™ä¹¦](https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book)å­¦Gané¡ºä¾¿å¤ä¹ äº†ä¸€éç»å…¸ç½‘ç»œï¼Œé¡ºä¾¿é™„ä¸Š[æå®æ¯…è®²è§£çš„Ganç½‘ç»œ](https://www.bilibili.com/video/BV1tE411Z78A?p=3)ï¼ˆæ¯æ¬¡çœ‹å®Œè¿™ç§è§†é¢‘éƒ½æ„Ÿè§‰æ¦‚ç‡è®ºç™½å­¦äº†ï¼Œå»ºè®®æå®æ¯…çš„å¯ä»¥å…ˆçœ‹ä¸€åŠå†çœ‹é¾™ä¹¦ã€‚

emmmï¼Œå†é™„ä¸Šåˆ«äººæ•´ç†çš„[æ·±åº¦å­¦ä¹ è·¯çº¿](https://leemeng.tw/deep-learning-resources.html)å§ ~~åº”è¯¥ä¸ä¼šæœ‰äººçœ‹çš„å®Œ~~