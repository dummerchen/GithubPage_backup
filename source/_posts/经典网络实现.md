---
title: 经典网络实现
mathjax: true
date: 2021.2.20
author: dummerfu
authorLink: dummerfu.tk
categories: 技术
comments: true
tags: 机器学习
photos: >-
  https://cdn.jsdelivr.net/gh/dummerchen/My_Image_Bed01@master/img/20210125234451.jpg
abbrlink: 35344
description:
keywords:
---

## 前言

<div class="tip error">不太喜欢看别人的代码,自己实现一遍经典网络,熟悉keras api</div>

 <div class="tip success"> 水篇博客 </div>

<div class="tip warning">没有训练测试过网络的效果,直接拿去用可能会出问题!!!</div>

本意是了解如何自己构建网络,以防日后的模型迁移要再学一遍. ~~不要问为什么我知道要重学~~

可能网络会有错误,但是无伤大雅,知道如何构建就行 ~~反正以后经典网络可以直接导入~~



## VGG

​	这里使用keras的高级api来构建网络,当然使用Sequential也可以实现同样的效果.

```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/19 20:48

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import datetime
import  tensorflow as tf
from  tensorflow import  keras


BATCHSIZE=32


def preprocess(x,y):
    print('pre',x.shape,y.shape)

    x=2*tf.cast(x,dtype=tf.float32)/255.0 -1
    y = tf.squeeze(y)
    y=tf.cast(y,dtype=tf.int32)
    y=tf.one_hot(y,depth=100)
    print('after:',x.shape,y.shape)
    return  x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def VGG(image_shape,n_class):

    print(image_shape[0],image_shape[1],image_shape[2])
    inputs = keras.Input(shape=[image_shape[0],image_shape[1],image_shape[2]])

    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(inputs)
    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x=keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Flatten()(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)

    outputs= keras.layers.Dense(n_class, activation=keras.activations.softmax, use_bias=True)(x)
    # 基于Model方法构建模型
    model = keras.Model(inputs=inputs, outputs=outputs)

    return model

def train(train_db,val_db,is_train=False):
    model = VGG([32,32,3],n_class=100)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(),
        metrics=['accuracy'],
    )

    path = os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db, validation_data=val_db, validation_freq=1, epochs=5, callbacks=[tensorboard])

        model.save_weights('./vgg16.h5')


(x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()
print('pre',x.shape,y.shape)

train_db=data2tensor(x,y)
test_db=data2tensor(x_test,y_test)

train(train_db=train_db,val_db=test_db,is_train=False)

```



## ResNet

不同的ResNet只有结构不同,unit是相同的只需要改变layer_dims就可以实现了

这里使用重写类来构建网络,虽然要写前向传播比较麻烦,但是自由度更高

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
from  tensorflow import keras
import tensorboard
from tensorflow.keras import layers
import datetime
from matplotlib import pyplot as plt


BATCHSIZE=64

class BasicBlock(layers.Layer):
    def __init__(self,filter_num,stride=1):
        super(BasicBlock,self).__init__()
        self.conv1=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=stride,padding='same')
        self.bn1=layers.BatchNormalization()
        self.relu=layers.Activation('relu')

        self.conv2=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=1,padding='same')
        self.bn2 = layers.BatchNormalization()

        if(stride!=1):
            self.downsample = keras.Sequential()
            self.downsample.add(layers.Conv2D(filters=filter_num,kernel_size=(1,1),strides=stride))
        else:
            self.downsample=lambda x:x

    def call(self,inputs,training=None):

        out=self.conv1(inputs)
        out=self.bn1(out)
        out=self.relu(out)
        out=self.conv2(out)
        out=self.bn2(out)

        identity=self.downsample(inputs)
        output=layers.add([out,identity])
        output=self.relu(output)
        return output

class ResNet(keras.Model):
    def __init__(self,layer_dims,num_classes=100):
        super(ResNet, self).__init__()
        # self.flatten=layers.Flatten(input_shape=(32,32,3))
        self.stem=keras.Sequential([
            layers.Conv2D(64,(3,3),strides=(1,1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='same')
        ])
        self.layer1=self.build_resblock(filter_num=64,blocks=layer_dims[0])
        self.layer2=self.build_resblock(filter_num=128,blocks=layer_dims[1],stride=2)
        self.layer3=self.build_resblock(filter_num=256,blocks=layer_dims[2],stride=2)
        self.layer4=self.build_resblock(filter_num=512,blocks=layer_dims[3],stride=2)

        self.avgpool=layers.GlobalAveragePooling2D()
        self.fc=layers.Dense(num_classes)

    def call(self,inputs,training=None):
        # x = tf.reshape(inputs, [-1, 32 * 32*3])
        out=self.stem(inputs)
        out=self.layer1(out)
        out=self.layer2(out)
        out=self.layer3(out)
        out=self.layer4(out)
        out=self.avgpool(out)
        out=self.fc(out)

        return out
    def build_resblock(self,filter_num,blocks,stride=1):
        res_block=keras.Sequential()
        res_block.add(BasicBlock(filter_num,stride))

        for _ in range(1,blocks):
            res_block.add(BasicBlock(filter_num,stride=1))
        return  res_block

def resnet18():
    return ResNet(layer_dims=[2,2,2,2])

def preprocess(x,y):
    print('pre:', x.shape, y.shape)
    x=tf.cast(x,dtype=tf.float32)/255.0
    y=tf.cast(y,dtype=tf.int32)
    y = tf.squeeze(y)
    y=tf.one_hot(y,depth=100)

    print('after', x.shape, y.shape)

    return x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def train_model(train_db,val_db,is_train=False):
    model = resnet18()

    model.build(input_shape=(None, 32, 32, 3))
    model.summary()
    x=tf.random.normal([4,32,32,3])
    out=model(x)
    print(out.shape)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'],
    )


    path=os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard=keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db,validation_data=val_db,validation_freq=1,epochs=5,callbacks=[tensorboard])

        model.save_weights('./resnet18.h5')


def main():
    (x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()

    l=int(len(x)*0.8)
    train_db=data2tensor(x[:l],y[:l])
    val_db=data2tensor(x[l:],y[l:])
    test_db=data2tensor(x_test,y_test)

    # sample=next(iter(train_db))
    # print(sample[0].shape,sample[1].shape)
    # plt.imshow(sample[0])
    # plt.show()
    train_model(train_db,val_db,is_train=False)

main()
```

## LSTM



<div class="tip warning">layers.lstmcell和layers.lstm传参是不一样的</div>

前者需要手动更新state参数($h_{t-1}$,$c_{t-1}$)但是后者自动更新，如果需要多层叠加则需要设置return_sequence=True , unroll=True



```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/21 17:46

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt

mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

import tensorflow as tf
from tensorflow import keras

# 最常见的前20000个单词
max_features=20000

# 一句话的最大长度
max_len=100
batchsize=64

class Mylstm(keras.Model):
    def __init__(self,units):
        super(Mylstm,self).__init__()

        # [b,100] => [b,100,100]

        self.embeding=keras.layers.Embedding(input_dim=max_features,input_length=max_len,output_dim=100)
        self.rnn=keras.Sequential([
            keras.layers.LSTM(units=units,dropout=0.5,return_sequences=True,unroll=True),
            keras.layers.LSTM(units=units,dropout=0.5,unroll=True)
        ])

        self.fc=keras.layers.Dense(1,activation=keras.activations.sigmoid)

    def call(self,inputs,training=None):

        # [b,100] => [b,100,100]
        x=self.embeding(inputs)
        print(x.shape)
        # [b,100,100] => [b,64]
        x=self.rnn(x)
        x=self.fc(x)

        return x


def data2tensor(x,y):
    x=keras.preprocessing.sequence.pad_sequences(sequences=x,maxlen=max_len)
    x=tf.cast(x,dtype=tf.int32)
    y=tf.cast(y,dtype=tf.int32)

    print(x.shape,y.shape)

    db=tf.data.Dataset.from_tensor_slices((x,y)).shuffle(10000).batch(batchsize,drop_remainder=True)
    return db

def train(db_train,db_val,db_test):

    model=Mylstm(64)
    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.BinaryCrossentropy(),
        metrics=['accuracy'],
    )
    model.fit(db_train,epochs=5,validation_data=db_val,validation_freq=1)

    model.evaluate(db_test)
    return

def main():
    (x,y),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=max_features)

    l=int(len(x)*0.8)
    db_train=data2tensor(x[:l],y[:l])
    db_val=data2tensor(x[l:],y[l:])
    db_test=data2tensor(x_test,y_test)


    train(db_train,db_val,db_test)


if __name__ == "__main__":
    main()

```

## AutoEncoder|VAE

*这里是自定义训练，当然相比之下更复杂但是自由度也更高。*

autoencoder就是两个自定义网络，先降维得到特征向量h，再升到原本维度就行了~~没什么技术含量，就不写了~~，关键是它的思路非常具有启发性。



这里要注意的是mean,var Dense是两个Dense，即使计算方式一样但是要用两Dense,如果一个Dense算两次因为权重的原因结果是相同的，直接会导致图片越来越暗。

先附上[tf.nn的几种损失函数区别](https://www.cnblogs.com/henuliulei/p/13742376.html)再附代码

```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/22 21:55

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import matplotlib as mpl
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from PIL import Image
import numpy as np
mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

tf.random.set_seed(2345)
# autoencoder 计算量很小batch可以大一点
batch_size=512
# 特征维数
z_dims=20


class VAE(keras.Model):
    def __init__(self):
        super(VAE,self).__init__()

        #encoder
        self.encoder=keras.Sequential([
            keras.layers.InputLayer(input_shape=(28*28)),
            keras.layers.Dense(128),
        ])

        self.meanfc=keras.layers.Dense(z_dims)
        self.varfc=keras.layers.Dense(z_dims)
        #decoder
        self.decoder=keras.Sequential([
            keras.layers.Dense(128, activation=tf.nn.relu),
            keras.layers.Dense(784),
        ])

    def reparamize(self,mean,log_var):

        eps=tf.random.normal(log_var.shape)
        z=mean+eps*tf.exp(log_var*0.5)
        return z

    def call(self,inputs,training=None):
        h=self.encoder(inputs)
        
        mean=self.meanfc(h)
        log_var=self.varfc(h)
        
        z=self.reparamize(mean,log_var)
        
        outputs=self.decoder(z)

        return outputs,mean,log_var

def data2tensor(x,y):

    x=tf.cast(x,dtype=tf.float32)/255.0
    db=tf.data.Dataset.from_tensor_slices(x)
    db=db.shuffle(batch_size*5).batch(batch_size)
    return db

def save_images(imgs,name):
    new_im = Image.new('L', (280, 280))

    index = 0
    for i in range(0, 280, 28):
        for j in range(0, 280, 28):
            im = imgs[index]
            im = Image.fromarray(im, mode='L')
            new_im.paste(im, (i, j))
            index += 1
    new_im.save(name)

def train_and_test(db_train,db_test):

    model=VAE()
    # model.build(input_shape=(4,784))
    optimizer=tf.optimizers.Adam()
    for epoch in range(100):
        for step,x in enumerate(db_train):
            # print(x.shape)
            x=tf.reshape(x,[-1,784])

            with tf.GradientTape() as tape:
                x_hat,mean,log_var=model(x)
                # 这里使用的这个loss是为了更好的收敛，使用其他的也行，但是要多训练
                redu_loss=tf.nn.sigmoid_cross_entropy_with_logits(x,x_hat)

                # 这里其实随便，reduce_mean(),reduce_sum()应该都行反正都是minimize loss
                # reduce_mean()和reduce_sum()|reduce_sum/x.shape[0]训练结果完全不同..
                # 但是后两者相似
                redu_loss=tf.reduce_sum(redu_loss)/x.shape[0]

                kl=-0.5*(log_var+1-mean**2-tf.exp(log_var))
                # prekl=tf.reduce_mean(kl)
                kl=tf.reduce_sum(kl)/x.shape[0]
     
                loss=redu_loss+kl*1.0
            grads=tape.gradient(loss,model.trainable_variables)

            optimizer.apply_gradients(zip(grads,model.trainable_variables))
            if step%50==0:

                print(epoch,step,"kl_loss:",kl,'loss:',loss,'x_shape0',x.shape[0])
        # evaluation
        z=tf.random.normal((batch_size,z_dims))
        sample_x=model.decoder(z)
        sample_x=tf.nn.sigmoid(sample_x)
        sample_x = tf.reshape(sample_x, [-1, 28, 28]).numpy() * 255.
        sample_x= sample_x.astype(np.uint8)

        save_images(sample_x, 'vae_images/sample_epoch_%d.png' % epoch)


        test_x = next(iter(db_test))
        test_x,_,_= model(tf.reshape(test_x, [-1, 784]))
        # [b, 784] => [b, 28, 28]
        test_x=tf.nn.sigmoid(test_x)
        test_x = tf.reshape(test_x, [-1, 28, 28])

        # [b, 28, 28] => [2b, 28, 28]
        test_x= test_x.numpy() * 255.
        test_x = test_x.astype(np.uint8)
        save_images(test_x, 'vae_images/test_epoch_%d.png' % epoch)

    model.save_weights('./vae.h5')

if __name__ == "__main__":
    (x,y),(x_test,y_test)=keras.datasets.mnist.load_data()

    l=int(len(x)*0.8)
    print(x.shape, y.shape,l,28*28)
    db_train=data2tensor(x[:l],y[:l])
    db_val=data2tensor(x[l:],y[l:])
    db_test=data2tensor(x_test,y_test)

    train_and_test(db_train,db_val)

```

## Gan

## 后记



​	本来是想每一个经典网络都详细写的，但是感觉这样会导致太专业全是公式也不会有人去仔细看 ~~其实是我不会。~~结果变成了现在这种类似板子的东西。~~水博客才是原动力~~



这里就随便总结一下：

### 代码方面

* keras.build(inputs_shape)：这里最好是使用tuple形式表示不然会报奇怪的错，tensorflow和pytorch不同这方面更加严格。
* tf.losses： 这个模块里的函数大小写不同功能也是不同的，~~具体可以看官网~~，如果用complie建议用大小的函数，自定义算loss使用小写的函数
* sigmod和softmax： 当'分类'事物不完全相互独立可以使用sigmod否则softmax，softmax一定要onehot 
* model.save：这个因为保存了网络结构只能用在纯自定义网络里，继承类是不行的。



### 网络等方面

​	可以从代码中看出现有的几种网络构建格式。当初我也纠结了许多，最后还是准备使用gan网络的格式，毕竟框架好用是好用，但这是牺牲‘自由’换来的，对后期自主构建网络可能会起到反效果。~~每个人喜好不同，也不用太参考我的建议。~~

​	k折验证等trick是视频里没有讲的（视频参考下面的学习资源），可以自己去看看相关trick。

### 学习资源

~~我才不是看到Gan可以随机生成老婆才想学Gan的~~

发现日月光华的《tensorflow入门学习与实战的》资源没弄到，免费课程讲的确实好。

就换成[龙书](https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book)学Gan顺便复习了一遍经典网络，顺便附上[李宏毅讲解的Gan网络](https://www.bilibili.com/video/BV1tE411Z78A?p=3)（每次看完这种视频都感觉概率论白学了

emmm，再附上别人整理的[深度学习路线](https://leemeng.tw/deep-learning-resources.html)吧 ~~应该不会有人看的完~~