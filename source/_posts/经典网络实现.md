---
title: 经典网络实现
mathjax: true
date: 2021.2.20
author: dummerfu
authorLink: dummerfu.tk
categories: 技术
comments: true
tags: 机器学习
photos: >-
  https://cdn.jsdelivr.net/gh/dummerchen/My_Image_Bed01@master/img/20210125234451.jpg
abbrlink: 35344
description:
keywords:
---

## 前言

<div class="tip error">不太喜欢看别人的代码,自己实现一遍经典网络,熟悉keras api</div>

 <div class="tip success"> 水篇博客 </div>

<div class="tip warning">没有训练测试过网络的效果,直接拿去用可能会出问题!!!</div>

本意是了解如何自己构建网络,以防日后的模型迁移要再学一遍. ~~不要问为什么我知道要重学~~

可能网络会有错误,但是无伤大雅,知道如何构建就行 ~~反正以后经典网络可以直接导入~~



## VGG

​	这里使用keras的高级api来构建网络,当然使用Sequential也可以实现同样的效果.

```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/19 20:48

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import datetime
import  tensorflow as tf
from  tensorflow import  keras


BATCHSIZE=32


def preprocess(x,y):
    print('pre',x.shape,y.shape)

    x=2*tf.cast(x,dtype=tf.float32)/255.0 -1
    y = tf.squeeze(y)
    y=tf.cast(y,dtype=tf.int32)
    y=tf.one_hot(y,depth=100)
    print('after:',x.shape,y.shape)
    return  x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def VGG(image_shape,n_class):

    print(image_shape[0],image_shape[1],image_shape[2])
    inputs = keras.Input(shape=[image_shape[0],image_shape[1],image_shape[2]])

    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(inputs)
    x=keras.layers.Conv2D(64, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x=keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(128, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(256, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.Conv2D(512, kernel_size=[3, 3], strides=[1, 1], activation=keras.activations.relu,padding='same')(x)
    x= keras.layers.MaxPooling2D(pool_size=[2, 2], strides=[2, 2], padding='same')(x)

    x= keras.layers.Flatten()(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)
    x= keras.layers.Dense(4096, activation=keras.activations.relu, use_bias=True)(x)

    outputs= keras.layers.Dense(n_class, activation=keras.activations.softmax, use_bias=True)(x)
    # 基于Model方法构建模型
    model = keras.Model(inputs=inputs, outputs=outputs)

    return model

def train(train_db,val_db,is_train=False):
    model = VGG([32,32,3],n_class=100)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(),
        metrics=['accuracy'],
    )

    path = os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db, validation_data=val_db, validation_freq=1, epochs=5, callbacks=[tensorboard])

        model.save_weights('./vgg16.h5')


(x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()
print('pre',x.shape,y.shape)

train_db=data2tensor(x,y)
test_db=data2tensor(x_test,y_test)

train(train_db=train_db,val_db=test_db,is_train=False)

```



## ResNet

不同的ResNet只有结构不同,unit是相同的只需要改变layer_dims就可以实现了

这里使用重写类来构建网络,虽然要写前向传播比较麻烦,但是自由度更高

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
from  tensorflow import keras
import tensorboard
from tensorflow.keras import layers
import datetime
from matplotlib import pyplot as plt


BATCHSIZE=64

class BasicBlock(layers.Layer):
    def __init__(self,filter_num,stride=1):
        super(BasicBlock,self).__init__()
        self.conv1=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=stride,padding='same')
        self.bn1=layers.BatchNormalization()
        self.relu=layers.Activation('relu')

        self.conv2=layers.Conv2D(kernel_size=(3,3) ,filters=filter_num,strides=1,padding='same')
        self.bn2 = layers.BatchNormalization()

        if(stride!=1):
            self.downsample = keras.Sequential()
            self.downsample.add(layers.Conv2D(filters=filter_num,kernel_size=(1,1),strides=stride))
        else:
            self.downsample=lambda x:x

    def call(self,inputs,training=None):

        out=self.conv1(inputs)
        out=self.bn1(out)
        out=self.relu(out)
        out=self.conv2(out)
        out=self.bn2(out)

        identity=self.downsample(inputs)
        output=layers.add([out,identity])
        output=self.relu(output)
        return output

class ResNet(keras.Model):
    def __init__(self,layer_dims,num_classes=100):
        super(ResNet, self).__init__()
        # self.flatten=layers.Flatten(input_shape=(32,32,3))
        self.stem=keras.Sequential([
            layers.Conv2D(64,(3,3),strides=(1,1)),
            layers.BatchNormalization(),
            layers.Activation('relu'),
            layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='same')
        ])
        self.layer1=self.build_resblock(filter_num=64,blocks=layer_dims[0])
        self.layer2=self.build_resblock(filter_num=128,blocks=layer_dims[1],stride=2)
        self.layer3=self.build_resblock(filter_num=256,blocks=layer_dims[2],stride=2)
        self.layer4=self.build_resblock(filter_num=512,blocks=layer_dims[3],stride=2)

        self.avgpool=layers.GlobalAveragePooling2D()
        self.fc=layers.Dense(num_classes)

    def call(self,inputs,training=None):
        # x = tf.reshape(inputs, [-1, 32 * 32*3])
        out=self.stem(inputs)
        out=self.layer1(out)
        out=self.layer2(out)
        out=self.layer3(out)
        out=self.layer4(out)
        out=self.avgpool(out)
        out=self.fc(out)

        return out
    def build_resblock(self,filter_num,blocks,stride=1):
        res_block=keras.Sequential()
        res_block.add(BasicBlock(filter_num,stride))

        for _ in range(1,blocks):
            res_block.add(BasicBlock(filter_num,stride=1))
        return  res_block

def resnet18():
    return ResNet(layer_dims=[2,2,2,2])

def preprocess(x,y):
    print('pre:', x.shape, y.shape)
    x=tf.cast(x,dtype=tf.float32)/255.0
    y=tf.cast(y,dtype=tf.int32)
    y = tf.squeeze(y)
    y=tf.one_hot(y,depth=100)

    print('after', x.shape, y.shape)

    return x,y

def data2tensor(x,y):

    db=tf.data.Dataset.from_tensor_slices((x,y))
    db=db.map(preprocess)
    db=db.shuffle(5000).batch(BATCHSIZE)

    return db

def train_model(train_db,val_db,is_train=False):
    model = resnet18()

    model.build(input_shape=(None, 32, 32, 3))
    model.summary()
    x=tf.random.normal([4,32,32,3])
    out=model(x)
    print(out.shape)

    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.CategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'],
    )


    path=os.path.abspath('./')
    log_dir = path + '\\logs\\' + datetime.datetime.now().strftime("%Y%m%d-%H%M")
    print(log_dir)
    tensorboard=keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)
    model.summary()
    if is_train:

        model.fit(train_db,validation_data=val_db,validation_freq=1,epochs=5,callbacks=[tensorboard])

        model.save_weights('./resnet18.h5')


def main():
    (x,y),(x_test,y_test)=keras.datasets.cifar100.load_data()

    l=int(len(x)*0.8)
    train_db=data2tensor(x[:l],y[:l])
    val_db=data2tensor(x[l:],y[l:])
    test_db=data2tensor(x_test,y_test)

    # sample=next(iter(train_db))
    # print(sample[0].shape,sample[1].shape)
    # plt.imshow(sample[0])
    # plt.show()
    train_model(train_db,val_db,is_train=False)

main()
```

## LSTM



<div class="tip warning">layers.lstmcell和layers.lstm传参是不一样的</div>

前者需要手动更新state参数($h_{t-1}$,$c_{t-1}$)但是后者自动更新，如果需要多层叠加则需要设置return_sequence=True , unroll=True



```python
# -*- coding:utf-8 -*-
# @Author : Dummerfu
# @Contact : https://github.com/dummerchen 
# @Time : 2021/2/21 17:46

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt

mpl.rcParams['font.sans-serif'] = 'SimHei'
mpl.rcParams['axes.unicode_minus'] = False

import tensorflow as tf
from tensorflow import keras

# 最常见的前20000个单词
max_features=20000

# 一句话的最大长度
max_len=100
batchsize=64

class Mylstm(keras.Model):
    def __init__(self,units):
        super(Mylstm,self).__init__()

        # [b,100] => [b,100,100]

        self.embeding=keras.layers.Embedding(input_dim=max_features,input_length=max_len,output_dim=100)
        self.rnn=keras.Sequential([
            keras.layers.LSTM(units=units,dropout=0.5,return_sequences=True,unroll=True),
            keras.layers.LSTM(units=units,dropout=0.5,unroll=True)
        ])

        self.fc=keras.layers.Dense(1,activation=keras.activations.sigmoid)

    def call(self,inputs,training=None):

        # [b,100] => [b,100,100]
        x=self.embeding(inputs)
        print(x.shape)
        # [b,100,100] => [b,64]
        x=self.rnn(x)
        x=self.fc(x)

        return x


def data2tensor(x,y):
    x=keras.preprocessing.sequence.pad_sequences(sequences=x,maxlen=max_len)
    x=tf.cast(x,dtype=tf.int32)
    y=tf.cast(y,dtype=tf.int32)

    print(x.shape,y.shape)

    db=tf.data.Dataset.from_tensor_slices((x,y)).shuffle(10000).batch(batchsize,drop_remainder=True)
    return db

def train(db_train,db_val,db_test):

    model=Mylstm(64)
    model.compile(
        optimizer=tf.optimizers.Adam(),
        loss=tf.losses.BinaryCrossentropy(),
        metrics=['accuracy'],
    )
    model.fit(db_train,epochs=5,validation_data=db_val,validation_freq=1)

    model.evaluate(db_test)
    return

def main():
    (x,y),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=max_features)

    l=int(len(x)*0.8)
    db_train=data2tensor(x[:l],y[:l])
    db_val=data2tensor(x[l:],y[l:])
    db_test=data2tensor(x_test,y_test)


    train(db_train,db_val,db_test)


if __name__ == "__main__":
    main()

```

